{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95c0efc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def manual_seed(seed=42):\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    # if you are suing GPU\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.enabled = False\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\n",
    "def get_model_size(model):\n",
    "\ttotal_size = sum(param.numel() for param in model.parameters() if param.requires_grad)\n",
    "\treturn total_size / 1e6\n",
    "\n",
    "manual_seed()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac10a513",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cffcd829",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import load_data, load_edge\n",
    "\n",
    "train_folds = load_data(True)\n",
    "test_fold = load_data(False)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "49da85ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "class SimpleStockDataset(Dataset):\n",
    "    def __init__(self, data, ws=128):\n",
    "        self.data = data\n",
    "        self.ws = ws\n",
    "        self.samples = []\n",
    "        \n",
    "        self.n_tickers, self.n_days, self.n_features = self.data.shape\n",
    "        \n",
    "        for start in range(self.n_days - self.ws + 1):\n",
    "            self.samples.append(start)\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "      \n",
    "    def __getitem__(self, idx):\n",
    "        start = self.samples[idx]\n",
    "        x = torch.tensor(self.data[:, start:start + self.ws], dtype=torch.float32)\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df3951fd",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b6f3b1c",
   "metadata": {},
   "source": [
    "### TemporalGraphRefiner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd467b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from typing import List, Optional, Tuple, Literal\n",
    "\n",
    "\n",
    "class MyTemporalEncoderLayerBiLSTM(nn.Module):\n",
    "    def __init__(self, input_size: int, hidden_size: int, n_layers: int = 1, dropout: float = 0.0, **kwargs):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size,\n",
    "            hidden_size,\n",
    "            n_layers,\n",
    "            batch_first=True,\n",
    "            dropout=(dropout if n_layers > 1 else 0.0),\n",
    "            bidirectional=True)\n",
    "\n",
    "        self.proj = nn.Linear(2 * hidden_size, hidden_size)\n",
    "\n",
    "    def forward(\n",
    "        self, x: torch.Tensor):\n",
    "        x, _ = self.lstm(x)\n",
    "        \n",
    "        x = self.proj(x)\n",
    "        return x\n",
    "\n",
    "class GraphChebMix(nn.Module):\n",
    "    def __init__(self, Pdeg: int, d_latent: int, safety: float = 0.99, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.Pdeg = Pdeg\n",
    "        self.coeffs = nn.Parameter(torch.randn(Pdeg + 1) * 0.05)\n",
    "        self.safety = safety\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, X: torch.Tensor, L: torch.Tensor):\n",
    "        \"\"\"\n",
    "        X: (N, H, d_latent); per-time graph mixing.\n",
    "        \"\"\"\n",
    "        Y = self.apply_cheb_seq(L, X, self.coeffs, safety=self.safety)\n",
    "        return self.dropout(Y)\n",
    "\n",
    "    @staticmethod\n",
    "    def apply_cheb_seq(L, X, coeffs, safety=0.99):\n",
    "        I_N = torch.eye(L.shape[0], device=L.device, dtype=L.dtype)\n",
    "        S = I_N - L\n",
    "\n",
    "        s = coeffs.abs().sum().clamp_min(1e-12)\n",
    "        scale = min(1.0, float(safety)) / float(s)\n",
    "        c = coeffs * scale\n",
    "\n",
    "        T0 = X\n",
    "        if c.numel() == 1:\n",
    "            return c[0] * T0\n",
    "        T1 = torch.einsum('ij,jhd->ihd', S, X)\n",
    "        Y  = c[0] * T0 + c[1] * T1\n",
    "        for j in range(2, c.numel()):\n",
    "            T2 = 2 * torch.einsum('ij,jhd->ihd', S, T1) - T0\n",
    "            Y  = Y + c[j] * T2\n",
    "            T0, T1 = T1, T2\n",
    "        return Y\n",
    "      \n",
    "\n",
    "class TemporalGraphLayer(nn.Module):\n",
    "    def __init__(self, d_latent: int = 128,\n",
    "                 Pdeg: int = 2, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.d_latent = d_latent\n",
    "        self.temporal_encoder = MyTemporalEncoderLayerBiLSTM(d_latent, d_latent)\n",
    "        self.graph_encoder = GraphChebMix(Pdeg=Pdeg, d_latent=d_latent, dropout=dropout)\n",
    "        self.ln_temporal = nn.LayerNorm(d_latent)\n",
    "        self.ln_graph = nn.LayerNorm(d_latent)\n",
    "        self.fuse = nn.Sequential(nn.Linear(2*d_latent, d_latent))\n",
    "        self.norm_out = nn.LayerNorm(d_latent)\n",
    "        \n",
    "\n",
    "    def forward(self, x, L):\n",
    "        \"\"\"\n",
    "        x:   (N, H, d_latent)\n",
    "        node_features: (N, d_node)\n",
    "        L: from your OUGCN\n",
    "        \"\"\"\n",
    "        nn.TransformerEncoderLayer\n",
    "        x_res = x\n",
    "        # temporal encoder\n",
    "        ht = self.ln_temporal(self.temporal_encoder(x)) # (N,H,d_latent)\n",
    "\n",
    "        # graph encoder\n",
    "        hg = self.ln_graph(self.graph_encoder(ht, L)) # (N,H,d_latent)\n",
    "\n",
    "        # fuse\n",
    "        h = self.fuse(torch.cat([ht, hg], dim=-1))                    # (N,H,d_latent)\n",
    "\n",
    "        out = self.norm_out(x_res + h)\n",
    "        \n",
    "        return out\n",
    "      \n",
    "\n",
    "class TemporalGraphRefiner(nn.Module):\n",
    "    def __init__(self, n_feats: int, d_node: int, d_latent: int = 128,\n",
    "                 num_layers: int = 2, \n",
    "                 Pdeg: int = 2, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.d_latent = d_latent\n",
    "        self.num_layers = num_layers\n",
    "        self.fc_in = nn.Linear(n_feats + d_node, d_latent)\n",
    "        self.temporal_graph_block = nn.ModuleList([\n",
    "            TemporalGraphLayer(self.d_latent, Pdeg, dropout)\n",
    "            for _ in range(self.num_layers)\n",
    "        ])\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(d_latent, d_latent), nn.GELU(),\n",
    "            nn.Linear(d_latent, n_feats)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, node_features, L):\n",
    "        \"\"\"\n",
    "        x:   (N, H, F)\n",
    "        node_features: (N, d_node)\n",
    "        L: from your OUGCN\n",
    "        \"\"\"\n",
    "        N, H, F = x.shape\n",
    "        device = x.device\n",
    "        \n",
    "        node_broadcast = node_features.unsqueeze(1).expand(N, H, node_features.size(-1))\n",
    "        x = self.fc_in(torch.cat([x, node_broadcast], dim=-1))  # (N,H,d_latent)\n",
    "\n",
    "        for _, block in enumerate(self.temporal_graph_block):\n",
    "            x = block(x, L)\n",
    "        \n",
    "        out = self.head(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da33d3b",
   "metadata": {},
   "source": [
    "### RNNBackbone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "13f67528",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from typing import List, Optional, Tuple, Literal\n",
    "\n",
    "MyRNNArchitectureType = Literal['lstm']\n",
    "\n",
    "class MyLSTMBackbone(nn.Module):\n",
    "    def __init__(self, input_size: int, hidden_size: int, n_layers: int, dropout: float, **kwargs):\n",
    "        super().__init__()\n",
    "        self.d_in = input_size\n",
    "        self.d_latent = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.cells = nn.ModuleList([\n",
    "            nn.LSTMCell(self.d_in if i == 0 else self.d_latent, self.d_latent)\n",
    "            for i in range(self.n_layers)\n",
    "        ])\n",
    "\n",
    "    def init_states(self, X_0: torch.Tensor, H_0: Optional[torch.Tensor] = None) -> Tuple[List[torch.Tensor], List[torch.Tensor]]:\n",
    "        \"\"\"\n",
    "        X_0: (N, d_in), H_0: (N, d_latent)\n",
    "        \"\"\"\n",
    "        device = X_0.device\n",
    "        dtype = X_0.dtype\n",
    "        N = X_0.shape[0]\n",
    "\n",
    "        h_list = [torch.zeros(N, self.d_latent, device=device, dtype=dtype) for _ in range(self.n_layers)]\n",
    "        c_list = [torch.zeros(N, self.d_latent, device=device, dtype=dtype) for _ in range(self.n_layers)]\n",
    "\n",
    "        if H_0 is not None:\n",
    "            if H_0.shape != (N, self.d_latent):\n",
    "                raise ValueError(f\"H_0 must be (N, {self.d_latent}), got {tuple(H_0.shape)}\")\n",
    "            h_list[-1] = H_0.to(device=device, dtype=dtype)\n",
    "\n",
    "        return h_list, c_list\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input: torch.Tensor,  # (N, d_in)\n",
    "        hx: Optional[Tuple[List[torch.Tensor], List[torch.Tensor]]] = None\n",
    "    ) -> Tuple[torch.Tensor, Tuple[List[torch.Tensor], List[torch.Tensor]]]:\n",
    "        if hx is None:\n",
    "            hx = self.init_states(input)\n",
    "        h_list, c_list = hx\n",
    "\n",
    "        new_h, new_c = [], []\n",
    "        inp = input\n",
    "        for l, cell in enumerate(self.cells):\n",
    "            h_t, c_t = cell(inp, (h_list[l], c_list[l]))\n",
    "            if self.training and self.dropout > 0:\n",
    "                h_t = F.dropout(h_t, p=self.dropout, training=True)\n",
    "            new_h.append(h_t)\n",
    "            new_c.append(c_t)\n",
    "            inp = h_t\n",
    "\n",
    "        h_top = new_h[-1]\n",
    "        return h_top, (new_h, new_c)\n",
    "\n",
    "\n",
    "class MyRNNCell(nn.Module):\n",
    "    def __init__(self, input_size: int, hidden_size: int, n_layers: int = 1, dropout: float = 0.0, **kwargs):\n",
    "        super().__init__()\n",
    "        arch_type = kwargs.get('arch_type', 'lstm')\n",
    "        backbone_kwargs = kwargs.get('backbone_kwargs', {})\n",
    "        assert arch_type in ['lstm'], f'Arch not supported: {arch_type}'\n",
    "\n",
    "        self.rnn = MyLSTMBackbone(input_size, hidden_size, n_layers, dropout, **backbone_kwargs)\n",
    "\n",
    "    @classmethod\n",
    "    def make(cls, **kwargs) -> 'MyRNNCell':\n",
    "        \"\"\"\n",
    "        Create Customized RNN Cell compatible with OUGCN.\n",
    "        \"\"\"\n",
    "        default_arch_type: MyRNNArchitectureType = 'lstm'\n",
    "        defaults = {\n",
    "            'n_layers': 1,\n",
    "            'dropout': 0.1,\n",
    "            'arch_type': default_arch_type,\n",
    "        }\n",
    "        return MyRNNCell(**(defaults | kwargs))\n",
    "\n",
    "    def init_states(self, X_0: torch.Tensor, H_0: Optional[torch.Tensor] = None):\n",
    "        return self.rnn.init_states(X_0, H_0)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input: torch.Tensor,\n",
    "        hx: Optional[Tuple[List[torch.Tensor], List[torch.Tensor]]] = None\n",
    "    ):\n",
    "        return self.rnn(input, hx)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6be37d6",
   "metadata": {},
   "source": [
    "### OUGCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1edb5714",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from __future__ import annotations\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, Tuple\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# ------------------------------\n",
    "# Graph utilities\n",
    "# ------------------------------\n",
    "\n",
    "def pearson_corr_matrix(series: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Compute Pearson correlation across time for N series.\n",
    "    Args:\n",
    "        series: (N, T) time-series (already aligned), not all-constant.\n",
    "    Returns:\n",
    "        corr: (N, N) in [-1,1]\n",
    "    \"\"\"\n",
    "    N, T = series.shape\n",
    "    x = series - series.mean(dim=1, keepdim=True)\n",
    "    std = x.std(dim=1, unbiased=False, keepdim=True) + 1e-8\n",
    "    x = x / std\n",
    "    corr = (x @ x.t()) / T\n",
    "    corr = corr.clamp(-1.0, 1.0)\n",
    "    return corr\n",
    "\n",
    "\n",
    "def build_adjacency_from_corr(corr: torch.Tensor,\n",
    "                              keep_negative: bool = False,\n",
    "                              knn: Optional[int] = None,\n",
    "                              threshold: Optional[float] = None) -> torch.Tensor:\n",
    "    \"\"\"Turn correlation into nonnegative adjacency.\n",
    "    - If keep_negative=False: use relu to zero-out negative correlations.\n",
    "    - Optional: keep only top-k neighbors per node (excluding self) or apply threshold.\n",
    "    Returns A with zero diagonal.\n",
    "    \"\"\"\n",
    "    N = corr.shape[0]\n",
    "    if not keep_negative:\n",
    "        A = corr.relu()\n",
    "    else:\n",
    "        # shift to nonnegative range [0, 2] then rescale to [0,1]\n",
    "        A = (corr + 1.0) / 2.0\n",
    "        A = A.clamp(0.0, 1.0)\n",
    "    eye = torch.eye(N, device=A.device, dtype=torch.bool)\n",
    "    A = A.masked_fill(eye, 0.0)\n",
    "\n",
    "    if threshold is not None:\n",
    "        A = torch.where(A >= threshold, A, torch.zeros_like(A))\n",
    "\n",
    "    if knn is not None and knn > 0 and knn < N:\n",
    "        # retain top-k per row (excluding diagonal already 0)\n",
    "        topk_vals, topk_idx = torch.topk(A, k=knn, dim=1)\n",
    "        mask = torch.zeros_like(A, dtype=torch.bool)\n",
    "        mask.scatter_(1, topk_idx, True)\n",
    "        A = torch.where(mask, A, torch.zeros_like(A))\n",
    "        # symmetrize by max\n",
    "        A = torch.max(A, A.t())\n",
    "\n",
    "    # final symmetry\n",
    "    A = 0.5 * (A + A.t())\n",
    "    A = A.masked_fill(eye, 0.0)\n",
    "    return A\n",
    "\n",
    "def normalized_adjacency(A):\n",
    "    \"\"\"Compute Atilde = Dtilde^{-1/2}(A+I)Dtilde^{-1/2}.\"\"\"\n",
    "    I = torch.eye(A.size(0), device=A.device, dtype=A.dtype)\n",
    "    Ahat = A + I\n",
    "    d = Ahat.sum(1) + 1e-8\n",
    "    dinv = d.pow(-0.5)\n",
    "    return dinv[:,None] * Ahat * dinv[None,:]\n",
    "\n",
    "def normalized_laplacian(A):\n",
    "    \"\"\"Compute L = I - D^{-1/2} A D^{-1/2}.\"\"\"\n",
    "    d = A.sum(dim=1) + 1e-8\n",
    "    dinv = d.pow(-0.5)\n",
    "    S = dinv[:,None] * A * dinv[None,:]\n",
    "    I = torch.eye(A.size(0), device=A.device, dtype=A.dtype)\n",
    "    return I - S\n",
    "\n",
    "def power_iteration_lmax_sym(M: torch.Tensor, n_iter: int = 25) -> float:\n",
    "    \"\"\"Estimate largest eigenvalue (spectral radius) of symmetric PSD matrix M.\"\"\"\n",
    "    N = M.shape[0]\n",
    "    v = torch.randn(N, device=M.device, dtype=M.dtype)\n",
    "    v = v / (v.norm() + 1e-8)\n",
    "    for _ in range(n_iter):\n",
    "        v = M @ v\n",
    "        n = v.norm() + 1e-8\n",
    "        v = v / n\n",
    "    # Rayleigh quotient\n",
    "    lmax = float((v @ (M @ v)).item())\n",
    "    return max(lmax, 1e-12)\n",
    "  \n",
    "def spec_norm_2(A: torch.Tensor, n_iter: int = 25) -> torch.Tensor:\n",
    "    \"\"\"||A||_2 via power method on A^T A. Returns a scalar Tensor (has grad wrt A).\"\"\"\n",
    "    AtA = A.T @ A\n",
    "    v = torch.randn(AtA.shape[0], device=A.device, dtype=A.dtype)\n",
    "    v = v / (v.norm() + 1e-8)\n",
    "    for _ in range(n_iter):\n",
    "        v = AtA @ v\n",
    "        v = v / (v.norm() + 1e-8)\n",
    "    # sqrt(v^T (A^T A) v) = ||A v||, but this Rayleigh ~ lambda_max; take sqrt.\n",
    "    lam_max = v @ (AtA @ v)\n",
    "    return lam_max.clamp_min(1e-12).sqrt()  # Tensor\n",
    "\n",
    "def compute_poly(L, coeffs):\n",
    "    I_N = torch.eye(L.shape[0], device=L.device, dtype=L.dtype)\n",
    "    return apply_poly_to_emb(L, I_N, coeffs)\n",
    "\n",
    "def compute_cheb(L, coeffs, safety=0.99):\n",
    "    I_N = torch.eye(L.shape[0], device=L.device, dtype=L.dtype)\n",
    "    K = apply_cheb_to_emb(L, I_N, coeffs, safety)\n",
    "    \n",
    "    # lam_max = spec_norm_2(K)\n",
    "    # scale = min(1.0, safety / lam_max)\n",
    "    # return K * scale\n",
    "    \n",
    "    return K\n",
    "\n",
    "def apply_poly_to_emb(L, V, coeffs) -> torch.Tensor:\n",
    "    \"\"\"Compute K * V = sum_{i=0}^P coeffs_i * L^i * V.\n",
    "    \"\"\"\n",
    "    out = coeffs[0] * V\n",
    "    if coeffs.numel() == 1:\n",
    "        return out\n",
    "    LV = V\n",
    "    for i in range(1, coeffs.numel()):\n",
    "        LV = L @ LV          # O(N^2 d)\n",
    "        out = out + coeffs[i] * LV\n",
    "    return out\n",
    "\n",
    "def apply_cheb_to_emb(L, V, coeffs, safety=0.99):\n",
    "    \"\"\"\n",
    "    Tính y = sum_j c_j T_j(S) V\n",
    "    S = I - L (phổ trong [-1,1]); V: (N,d) hoặc (N,F)\n",
    "    \"\"\"\n",
    "    I_N = torch.eye(L.shape[0], device=L.device, dtype=L.dtype)\n",
    "    S = I_N - L\n",
    "\n",
    "    c = clamp_l1(coeffs, safety)\n",
    "    \n",
    "    if c.numel() == 1:\n",
    "        return c[0] * V\n",
    "    T0 = V\n",
    "    T1 = S @ V              # T1 = T_1(S)V\n",
    "    y  = c[0] * T0 + c[1] * T1\n",
    "    for j in range(2, c.numel()):\n",
    "        T2 = 2 * (S @ T1) - T0\n",
    "        y = y + c[j] * T2\n",
    "        T0, T1 = T1, T2\n",
    "    return y\n",
    "\n",
    "def clamp_l1(coeffs, safety=0.99):\n",
    "    s = coeffs.abs().sum().clamp_min(1e-12)\n",
    "    scale = torch.minimum(torch.tensor(1.0, device=coeffs.device, dtype=coeffs.dtype), safety / s)\n",
    "    return coeffs * scale\n",
    "\n",
    "# ------------------------------\n",
    "# Model definition\n",
    "# ------------------------------\n",
    "\n",
    "import math\n",
    "\n",
    "class MyGCN(nn.Module):\n",
    "    def __init__(self, in_feats, out_feats, activation=None, bias=True, dropout=0):\n",
    "        super(MyGCN, self).__init__()\n",
    "        self.dropout = dropout\n",
    "        self.activation = activation\n",
    "        \n",
    "        self.weight = nn.Parameter(torch.FloatTensor(in_feats, out_feats))\n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.FloatTensor(out_feats))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        stdv = 1. / math.sqrt(self.weight.size(1))\n",
    "        self.weight.data.uniform_(-stdv, stdv)\n",
    "        if self.bias is not None:\n",
    "            self.bias.data.uniform_(-stdv, stdv)\n",
    "\n",
    "    def graph_convolve(self, x, adj):\n",
    "        support = x @ self.weight\n",
    "        output = adj @ support\n",
    "        if self.bias is not None:\n",
    "            return output + self.bias\n",
    "        else:\n",
    "            return output\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        x = self.graph_convolve(x, adj)\n",
    "        if self.activation is not None:\n",
    "            x = self.activation(x)\n",
    "        if self.training and self.dropout > 0:\n",
    "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        return x\n",
    "\n",
    "class GatedCell(nn.Module):\n",
    "    def __init__(self, d_latent):\n",
    "        super().__init__()\n",
    "        self.z_gate = nn.Linear(2*d_latent, d_latent)\n",
    "        nn.init.constant_(self.z_gate.bias, -1.0)\n",
    "        self.proj_A = nn.Sequential(\n",
    "            nn.LayerNorm(d_latent),\n",
    "            nn.Linear(d_latent, d_latent),\n",
    "            nn.GELU(),\n",
    "        )\n",
    "        self.proj_B = nn.Sequential(\n",
    "            nn.LayerNorm(d_latent),\n",
    "            nn.Linear(d_latent, d_latent),\n",
    "            nn.GELU(),\n",
    "        )\n",
    "\n",
    "    def forward(self, A, B):\n",
    "        z = torch.sigmoid(self.z_gate(torch.cat([self.proj_A(A), self.proj_B(B)], dim=-1)))\n",
    "        \n",
    "        A = (1 - z) * A + z * B\n",
    "        return A\n",
    "\n",
    "class OUGCN_with_Refiner(nn.Module):\n",
    "    def __init__(self, n_nodes: int, n_feats: int, args, node_emb=None):\n",
    "        super().__init__()\n",
    "        self.n_nodes = n_nodes\n",
    "        self.rank_adj = args.rank_adj\n",
    "        self.top_k_adj = args.top_k_adj\n",
    "        self.n_feats = n_feats\n",
    "        self.args = args\n",
    "        d_latent = args.d_latent\n",
    "        d_node = args.d_node\n",
    "        in_dropout = args.in_dropout\n",
    "        \n",
    "        rnn_kwargs:dict = getattr(args, 'rnn_kwargs', {})\n",
    "        \n",
    "        rnn_arch_type = rnn_kwargs.setdefault('rnn_arch_type', 'lstm')\n",
    "        rnn_n_layers = rnn_kwargs.setdefault('rnn_n_layers', 1)\n",
    "        rnn_hidden_dim = rnn_kwargs.setdefault('rnn_hidden_dim', 128)\n",
    "        rnn_dropout = rnn_kwargs.setdefault('rnn_dropout', 0.0)\n",
    "        rnn_backbone_kwargs = rnn_kwargs.setdefault('rnn_backbone_kwargs', {})\n",
    "        \n",
    "\n",
    "        if node_emb is None:\n",
    "            self.static_node_features = nn.Parameter(torch.randn(n_nodes, d_node) * 0.1)\n",
    "        else:\n",
    "            self.static_node_features = nn.Parameter(node_emb, requires_grad=False)\n",
    "            d_node = self.static_node_features.shape[-1]\n",
    "            \n",
    "        \n",
    "        self.fc_in = nn.Linear(n_feats + d_node, d_latent)\n",
    "        self.gcn_in = MyGCN(d_latent, d_latent, F.relu, dropout=in_dropout)\n",
    "        self.rnn_mean = MyRNNCell.make(\n",
    "            input_size=n_feats, hidden_size=rnn_hidden_dim,\n",
    "            n_layers=rnn_n_layers, dropout=rnn_dropout,\n",
    "            arch_type=rnn_arch_type,\n",
    "            backbone_kwargs=rnn_backbone_kwargs)\n",
    "        self.gcn_mean = MyGCN(rnn_hidden_dim, d_latent, F.relu, dropout=in_dropout)\n",
    "        self.fc_mean = nn.Linear(rnn_hidden_dim, d_latent)\n",
    "        \n",
    "        self.readout = nn.Sequential(\n",
    "            nn.Linear(d_latent, d_latent), nn.GELU(),\n",
    "            nn.Linear(d_latent, n_feats), nn.Tanh(),\n",
    "        )\n",
    "        self.res_scale = nn.Parameter(torch.ones(n_feats))\n",
    "        \n",
    "        self.kappa_H = nn.Parameter(torch.randn(args.Pdeg + 1) * 0.1)\n",
    "        self.kappa_M = nn.Parameter(torch.randn(args.Pdeg + 1) * 0.1)\n",
    "        \n",
    "        self.H_mix_module = GatedCell(d_latent)\n",
    "        self.M_mix_module = GatedCell(d_latent)\n",
    "        \n",
    "        self.fc_corr_features = nn.Sequential(\n",
    "            nn.Linear(d_node, d_latent), nn.ReLU(),\n",
    "            nn.Linear(d_latent, self.rank_adj)\n",
    "        )\n",
    "        # self.fc_node_features = nn.Sequential(\n",
    "        #     nn.Linear(d_node, d_latent), nn.ReLU(),\n",
    "        #     nn.Linear(d_latent, d_latent),\n",
    "        # )\n",
    "        \n",
    "        self.refiner = TemporalGraphRefiner(n_feats, d_node, d_latent,\n",
    "                                            **args.refiner_kwargs)\n",
    "        \n",
    "        self.corr_features = None\n",
    "        self.node_features = None\n",
    "        \n",
    "    def build_graph(self, X: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Build A (adj), Atilde (norm adj for GCN), and L (normalized Laplacian) from data X.\n",
    "        Right now we build a data-independent static learnable adj matrix.\n",
    "        X: (N, T, F)\n",
    "        Returns: (A, Atilde, L) all in device/dtype of X.\n",
    "        \"\"\"\n",
    "        # n_nodes, n_steps, n_feats = X.shape\n",
    "        if self.corr_features is None:\n",
    "            self.corr_features = self.fc_corr_features(self.static_node_features)\n",
    "        self.corr_features = F.normalize(self.corr_features, dim=-1)\n",
    "        A = self.corr_features @ self.corr_features.T\n",
    "        # A.fill_diagonal_(0.0)\n",
    "        A = build_adjacency_from_corr(A, keep_negative=False, knn=self.top_k_adj)\n",
    "        Atilde = normalized_adjacency(A)\n",
    "        L = normalized_laplacian(A)\n",
    "        return A, Atilde, L\n",
    "      \n",
    "\n",
    "    def _compute_graph_ops(self, X: torch.Tensor):\n",
    "        \"\"\"Helper: build Atilde, L, K, identity.\"\"\"\n",
    "        device = self.args.device\n",
    "        X = X.to(next(self.parameters()).device)\n",
    "        _, Atilde, L = self.build_graph(X)\n",
    "        Atilde = Atilde.to(device)\n",
    "        L = L.to(device)\n",
    "        \n",
    "        return Atilde, L\n",
    "    \n",
    "    def _step(self, X_t, H_t, H_filter, M_filter, gcn_filter, rnn_mean_latent=None):\n",
    "        Z_t_hist, rnn_mean_latent = self.rnn_mean.forward(X_t, rnn_mean_latent)\n",
    "        \n",
    "        M_t = self.fc_mean(Z_t_hist) + self.gcn_mean(Z_t_hist, gcn_filter) # mean embedding\n",
    "        \n",
    "        H_cand = self.H_mix_module(H_t, H_filter @ H_t)\n",
    "        M_cand = self.M_mix_module(M_t, M_filter @ M_t)\n",
    "        H_next = H_cand + M_cand\n",
    "        r_t = self.readout(H_next) * self.res_scale  # (N, F)\n",
    "        \n",
    "        return r_t, H_next, rnn_mean_latent\n",
    "\n",
    "    \n",
    "    \n",
    "    def forecast(self, X: torch.Tensor,\n",
    "                H_0: Optional[torch.Tensor] = None,\n",
    "                horizon: int = 0) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        y_pred, *_ = self.forward(X, H_0, horizon)\n",
    "        return y_pred\n",
    "    \n",
    "    def forward(self, X: torch.Tensor,\n",
    "                H_0: Optional[torch.Tensor] = None,\n",
    "                horizon: int = 0) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"Run deterministic inference.\n",
    "        Args:\n",
    "            X: (N, T, F) input features at each t\n",
    "            H_0: (N, d) optional initial latent state (default zeros)\n",
    "            Atilde, L: if precomputed; otherwise build from X\n",
    "        Returns:\n",
    "            y_pred: (N, T-1) predicted next-day log-return for each node\n",
    "            H_all: (N, T, d) latent states (including H_0 as first)\n",
    "        \"\"\"\n",
    "        device = self.args.device\n",
    "        X = X.to(next(self.parameters()).device)\n",
    "        n_nodes, n_steps, n_feats = X.shape\n",
    "        d_latent = self.args.d_latent\n",
    "        \n",
    "        self.corr_features = self.fc_corr_features(self.static_node_features)\n",
    "        # self.node_features = self.fc_node_features(self.static_node_features)\n",
    "        \n",
    "\n",
    "        Atilde, L = self._compute_graph_ops(X)\n",
    "        I_N = torch.eye(L.shape[0], device=L.device, dtype=L.dtype)\n",
    "        \n",
    "        \n",
    "        self.L = L\n",
    "\n",
    "        X_0 = X[:, 0, :] # (N, F)\n",
    "        if H_0 is None:\n",
    "            x_in = self.fc_in(torch.cat([X_0, self.static_node_features], dim=-1)) # (N, F + d_node)\n",
    "            H_t = self.gcn_in(x_in, Atilde)\n",
    "        else:\n",
    "            H_t = H_0.to(device)\n",
    "        \n",
    "        # H_t = H_t + self.node_features\n",
    "\n",
    "        H_all = torch.zeros(n_nodes, n_steps + max(horizon, 0), d_latent, device=device, dtype=X.dtype)\n",
    "        r_all = torch.zeros(n_nodes, n_steps - 1 + max(horizon, 0), n_feats, device=device, dtype=X.dtype)\n",
    "        y_all = torch.zeros(n_nodes, n_steps - 1 + max(horizon, 0), n_feats, device=device, dtype=X.dtype)\n",
    "\n",
    "        H_all[:, 0] = H_t\n",
    "        \n",
    "        H_filter = compute_cheb(L, self.kappa_H)\n",
    "        M_filter = I_N - Atilde\n",
    "        gcn_filter = compute_cheb(L, self.kappa_M)\n",
    "        \n",
    "        rnn_mean_latent = self.rnn_mean.init_states(X_0, H_t)\n",
    "        \n",
    "        for t in range(n_steps - 1):\n",
    "            X_t = X[:, t, :] # (N, D)\n",
    "            r_t, H_next, rnn_mean_latent = self._step(X_t, H_t, H_filter, M_filter, gcn_filter, rnn_mean_latent)\n",
    "\n",
    "            H_all[:, t + 1] = H_next\n",
    "            r_all[:, t] = r_t\n",
    "            y_all[:, t] = X_t + r_t\n",
    "            H_t = H_next\n",
    "            \n",
    "        if horizon > 0:\n",
    "            X_t = X[:, -1, :]\n",
    "            for s in range(horizon):\n",
    "                r_t, H_next, rnn_mean_latent = self._step(X_t, H_t, H_filter, M_filter, gcn_filter, rnn_mean_latent)\n",
    "\n",
    "                H_all[:, n_steps + s] = H_next\n",
    "                r_all[:, n_steps - 1 + s] = r_t\n",
    "                H_t = H_next\n",
    "                \n",
    "                y_all[:, n_steps - 1 + s] = X_t + r_t\n",
    "                X_t = X_t + r_t\n",
    "        \n",
    "        r_refine = self.refiner.forward(y_all, self.static_node_features, L)\n",
    "        y_refine = y_all + r_refine\n",
    "        \n",
    "        return y_refine, r_refine, y_all, r_all, H_all\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def forward_loss(\n",
    "        self,\n",
    "        X: torch.Tensor,\n",
    "        H_0: Optional[torch.Tensor] = None,\n",
    "        horizon: int = 0,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Tính loss dự báo one-step + rollout horizon (autoregressive) trên chuỗi đầu vào.\n",
    "\n",
    "        Args:\n",
    "            X: (N, T, F) chuỗi gốc (chứa full ground-truth đến T-1)\n",
    "            H_0: (N, d) latent init (tuỳ chọn)\n",
    "            horizon: số bước rollout ngoài quan sát cuối cùng\n",
    "                    (nếu >0, ta cắt input để tránh nhìn thấy tương lai)\n",
    "            reduction: 'mean' | 'sum' | 'none' cho F.mse_loss\n",
    "\n",
    "        Returns:\n",
    "            loss: scalar tensor\n",
    "            y_pred: (N, T-1, F) dự báo X_{t+1} cho toàn bộ t=0..T-2\n",
    "        \"\"\"\n",
    "        device = next(self.parameters()).device\n",
    "        X = X.to(device)\n",
    "        N, T, Fdim = X.shape\n",
    "\n",
    "        if horizon < 0:\n",
    "            raise ValueError(\"horizon must be >= 0\")\n",
    "        if horizon >= T:\n",
    "            raise ValueError(f\"horizon={horizon} must be < sequence length T={T}\")\n",
    "\n",
    "        # Cắt input nếu rollout > 0 để giữ đúng số target (T-1)\n",
    "        X_in = X[:, : T - horizon, :] if horizon > 0 else X\n",
    "\n",
    "        # forward() trả (N, (T-horizon)-1 + horizon, F) = (N, T-1, F)\n",
    "        y_refine, _, y_ar, _, _ = self.forward(X_in, H_0=H_0, horizon=horizon)\n",
    "\n",
    "        # Ground truth luôn là X_{1:T}\n",
    "        target = X[:, 1:, :]  # (N, T-1, F)\n",
    "        \n",
    "        err_t = (y_ar - target).abs().mean(dim=-1).mean(dim=0)\n",
    "        \n",
    "        decay = 0.9\n",
    "        coef_pre = 1.0\n",
    "        coef_roll = 1.0\n",
    "        \n",
    "        len_pre = T - horizon - 1\n",
    "        loss_pre = torch.tensor(0.0, device=device, dtype=err_t.dtype)\n",
    "        loss_roll = torch.tensor(0.0, device=device, dtype=err_t.dtype)\n",
    "\n",
    "        if len_pre > 0:\n",
    "            idx = torch.arange(len_pre - 1, -1, -1, device=device, dtype=err_t.dtype)\n",
    "            w = decay ** idx\n",
    "            loss_pre = (w * err_t[:len_pre]).sum() / (w.sum() + 1e-12)\n",
    "\n",
    "        if horizon > 0:\n",
    "            # MSE đều cho đoạn rollout (chiều dài = horizon)\n",
    "            loss_roll = err_t[len_pre:].mean()\n",
    "        \n",
    "        loss_refine = (y_refine - target).abs().mean()\n",
    "\n",
    "        loss = loss_refine + coef_pre * loss_pre + coef_roll * loss_roll\n",
    "        \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd4f0de",
   "metadata": {},
   "source": [
    "# Eval & Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5a8ff2d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import *\n",
    "\n",
    "def eval_ensemble(args, model, training_data_np, testing_data_np, device='cuda', seq_lens=[64, 96, 128], verbose=False):\n",
    "\n",
    "    labels = torch.tensor(testing_data_np).float().to(device)\n",
    "    n_nodes, horizon, n_feats = labels.shape\n",
    "    y_preds = np.zeros((len(seq_lens), n_nodes, horizon, n_feats-1)) # Bỏ Vol\n",
    "    model.eval().to(device)\n",
    "    for i, seq_len in enumerate(seq_lens):\n",
    "        batch = torch.tensor(training_data_np[:, -seq_len:]).float().to(device)\n",
    "        with torch.no_grad():\n",
    "            y_all = model.forecast(batch, horizon=horizon)\n",
    "        y_preds[i] = y_all[:, -horizon:, :n_feats-1].detach().cpu().numpy() # OHLC + Adj Close\n",
    "    y_gt = testing_data_np[:, :, :n_feats-1].reshape(n_nodes, -1) # OHLC + Adj Close\n",
    "    y_pred = y_preds.mean(axis=0).reshape(n_nodes, -1)\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"Max var:\", np.var(np.expm1(y_preds), axis=0).max())\n",
    "        print(\"Mean var:\", np.var(np.expm1(y_preds), axis=0).mean())\n",
    "    return {\n",
    "        'rmse': root_mean_squared_error(y_gt, y_pred), \n",
    "        'raw_rmse': root_mean_squared_error(np.expm1(y_gt), np.expm1(y_pred)), \n",
    "        'mae': mean_absolute_error(y_gt, y_pred), \n",
    "        'raw_mae': mean_absolute_error(np.expm1(y_gt), np.expm1(y_pred)), \n",
    "        'r2': r2_score(y_gt.ravel(), y_pred.ravel()),\n",
    "        'raw_r2': r2_score(np.expm1(y_gt).ravel(), np.expm1(y_pred).ravel()), \n",
    "    }\n",
    "\n",
    "def eval(args, model, training_data_np, testing_data_np, device='cuda'):\n",
    "    return eval_ensemble(args, model, training_data_np, testing_data_np, device, [args.seq_len])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fafc31ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\n",
    "       Imported from https://github.com/pytorch/examples/blob/master/imagenet/main.py#L247-L262\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        if self.avg == 0:\n",
    "            self.avg = val\n",
    "            return\n",
    "        self.avg = 0.95 * self.avg + 0.05 * val\n",
    "\n",
    "import copy\n",
    "def train(args, train_loader, model, optimizer, scheduler, training_data_np, testing_data_np):\n",
    "    # if args.amp:\n",
    "    #     from apex import amp\n",
    "    global best_loss, best_model\n",
    "    test_losses = []\n",
    "    end = time.time()\n",
    "\n",
    "    best_model = copy.deepcopy(model)\n",
    "    step = 0\n",
    "    for epoch in range(args.epochs):\n",
    "        batch_time = AverageMeter()\n",
    "        data_time = AverageMeter()\n",
    "        losses = AverageMeter()\n",
    "        stats01 = AverageMeter()\n",
    "        stats02 = AverageMeter()\n",
    "        p_bar = tqdm(train_loader)\n",
    "        for batch_idx, samples in enumerate(p_bar):\n",
    "            step += 1\n",
    "            model.train().to(args.device)\n",
    "          \n",
    "            samples = samples[0].float().to(args.device)\n",
    "            data_time.update(time.time() - end)\n",
    "\n",
    "            loss = model.forward_loss(samples, horizon=args.horizon)\n",
    "            \n",
    "            loss.backward()\n",
    "            \n",
    "            max_norm = 5.0\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm).item()\n",
    "            \n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            losses.update(loss.item())\n",
    "            stats01.update(0.0)\n",
    "            # stats01.update(power_iteration_lmax_sym(poly_laplacian(model.L, F.softplus(model.kappa))))\n",
    "\n",
    "            batch_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "            # mask_probs.update(mask.mean().item())\n",
    "            p_bar.set_description(\n",
    "                \"Ep: {epoch}/{epochs:3}. LR: {lr:.3e}. \"\n",
    "                \"Loss: {loss:.4f}. Stats01: {stats01:.4f}\".format(\n",
    "                epoch=epoch + 1,\n",
    "                epochs=args.epochs,\n",
    "                lr=scheduler.get_last_lr()[0],\n",
    "                data=data_time.avg,\n",
    "                bt=batch_time.avg,\n",
    "                loss=losses.avg,\n",
    "                stats01=stats01.avg,\n",
    "            ))\n",
    "            p_bar.update()\n",
    "            \n",
    "            if (step + 1) % args.eval_steps == 0:\n",
    "                test_model = model\n",
    "\n",
    "                test_metrics = eval_ensemble(args, test_model, training_data_np, testing_data_np, args.device, [args.seq_len])\n",
    "                print(test_metrics)\n",
    "                test_loss = test_metrics['mae']\n",
    "\n",
    "                is_best = test_loss < best_loss\n",
    "                if test_loss < best_loss:\n",
    "                    best_loss = test_loss\n",
    "                    best_model = copy.deepcopy(test_model)\n",
    "\n",
    "\n",
    "                test_losses.append(test_loss)\n",
    "                print('Best loss: {:.3f}'.format(best_loss))\n",
    "                print('Mean loss: {:.3f}\\n'.format(\n",
    "                    np.mean(test_losses[-20:])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3c4bc09d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "\n",
    "def get_cosine_schedule_with_warmup(optimizer,\n",
    "                                    num_warmup_steps,\n",
    "                                    num_training_steps,\n",
    "                                    num_cycles=7./16.,\n",
    "                                    last_epoch=-1):\n",
    "    def _lr_lambda(current_step):\n",
    "        if current_step < num_warmup_steps:\n",
    "            return float(current_step) / float(max(1, num_warmup_steps))\n",
    "        no_progress = float(current_step - num_warmup_steps) / \\\n",
    "            float(max(1, num_training_steps - num_warmup_steps))\n",
    "        return max(0., math.cos(math.pi * num_cycles * no_progress))\n",
    "\n",
    "    return LambdaLR(optimizer, _lr_lambda, last_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d6ce9a9",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c25b9810",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def manual_seed(seed=42):\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    # if you are suing GPU\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.enabled = False\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\n",
    "def get_model_size(model):\n",
    "\ttotal_size = sum(param.numel() for param in model.parameters() if param.requires_grad)\n",
    "\treturn total_size / 1e6\n",
    "\n",
    "class Config:\n",
    "    # Training\n",
    "    epochs = 5\n",
    "    eval_steps = 200\n",
    "    lr = 1e-4\n",
    "    wd = 1e-3\n",
    "    warmup = 0\n",
    "    \n",
    "    n_nodes = 428\n",
    "    n_feats = 6\n",
    "    \n",
    "    # Prediction\n",
    "    seq_len = 96\n",
    "    horizon = 32\n",
    "    \n",
    "    # OUGCN\n",
    "    d_node: int = 32\n",
    "    in_dropout: float = 0.0\n",
    "    d_latent: int = 128\n",
    "    Pdeg: int = 2                  # polynomial degree of Laplacian in K\n",
    "    safety: float = 0.99           # stability safety factor\n",
    "    device: str = \"cuda\"\n",
    "    top_k_adj: int = 32\n",
    "    rank_adj: int = 32\n",
    "    \n",
    "    # RNN kwargs\n",
    "    rnn_kwargs = {\n",
    "        'rnn_arch_type': 'lstm',\n",
    "        'rnn_n_layers': 1,\n",
    "        'rnn_dropout': 0.0,\n",
    "        'rnn_hidden_dim': 128,\n",
    "        'rnn_backbone_kwargs': { },\n",
    "    }\n",
    "    \n",
    "    # Refiner kwargs\n",
    "    refiner_kwargs = {\n",
    "        'num_layers': 1,\n",
    "        'Pdeg': 2,\n",
    "        'dropout': 0.1,\n",
    "    }\n",
    "    seed = 42\n",
    "\n",
    "args = Config()\n",
    "manual_seed(args.seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0adbbaf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "node_emb_path=\"../input/static_node_emb.npy\"\n",
    "node_emb_matrix = torch.tensor(np.load(node_emb_path), dtype=torch.float32)\n",
    "\n",
    "\n",
    "def train_fold(fold_idx):\n",
    "    global training_data_np, testing_data_np\n",
    "    used_features = [0, 1, 2, 3, 4, 5]\n",
    "    training_data_np = np.log1p(train_folds[fold_idx][0][:, :, used_features])\n",
    "    testing_data_np = np.log1p(train_folds[fold_idx][1][:, :, used_features])\n",
    "    train_loader = DataLoader(SimpleStockDataset(training_data_np, args.seq_len + args.horizon), batch_size=1, shuffle=True)\n",
    "    \n",
    "    manual_seed(args.seed + fold_idx)\n",
    "        \n",
    "    model = OUGCN_with_Refiner(args.n_nodes, args.n_feats, args, node_emb=node_emb_matrix)\n",
    "\n",
    "    from torch.optim import AdamW\n",
    "\n",
    "    optimizer = AdamW(model.parameters(), lr=args.lr, weight_decay=args.wd)\n",
    "    total_steps = args.epochs * len(train_loader)\n",
    "    scheduler = get_cosine_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=args.warmup,\n",
    "        num_training_steps=total_steps,\n",
    "    )\n",
    "    \n",
    "    print(f'Model size: {get_model_size(model) * 1e3:.2f}K')\n",
    "    \n",
    "    # Sanity check\n",
    "    print(eval_ensemble(args, model, training_data_np, testing_data_np, args.device, [2, 4]))\n",
    "    \n",
    "    global best_loss, best_model\n",
    "    best_loss = 9999\n",
    "    best_model = copy.deepcopy(model)\n",
    "\n",
    "    train(args, train_loader, model, optimizer, scheduler, training_data_np, testing_data_np)\n",
    "    \n",
    "    return best_model, best_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b49c3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model size: 630.33K\n",
      "{'rmse': 0.380309881486751, 'raw_rmse': 579.2165059396674, 'mae': 0.3428282398365135, 'raw_mae': 129.83877709089793, 'r2': 0.775560176849138, 'raw_r2': -6.986756850649582}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep: 1/  5. LR: 9.999e-05. Loss: 0.3201. Stats01: 0.0000:   6%|▌         | 219/3529 [02:14<36:04,  1.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rmse': 0.10707334198143661, 'raw_rmse': 43.942521752764236, 'mae': 0.09157104603100927, 'raw_mae': 19.235482651270896, 'r2': 0.984412341705579, 'raw_r2': 0.9903683831264545}\n",
      "Best loss: 0.092\n",
      "Mean loss: 0.092\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep: 1/  5. LR: 9.995e-05. Loss: 0.3168. Stats01: 0.0000:  12%|█▏        | 427/3529 [04:37<48:12,  1.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rmse': 0.09551845733961495, 'raw_rmse': 42.956224472408515, 'mae': 0.07985732003660735, 'raw_mae': 17.670086399113746, 'r2': 0.9878442000839696, 'raw_r2': 0.9911661504642908}\n",
      "Best loss: 0.080\n",
      "Mean loss: 0.086\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep: 1/  5. LR: 9.989e-05. Loss: 0.3016. Stats01: 0.0000:  18%|█▊        | 634/3529 [07:44<39:12,  1.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rmse': 0.08099990546139797, 'raw_rmse': 33.2661863122052, 'mae': 0.06252817886883644, 'raw_mae': 13.569174981319957, 'r2': 0.9906760188461735, 'raw_r2': 0.9948468407728508}\n",
      "Best loss: 0.063\n",
      "Mean loss: 0.078\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep: 1/  5. LR: 9.981e-05. Loss: 0.2841. Stats01: 0.0000:  24%|██▍       | 839/3529 [10:51<41:23,  1.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rmse': 0.08456490343212174, 'raw_rmse': 49.92634166349647, 'mae': 0.0671057458812477, 'raw_mae': 16.530334689194426, 'r2': 0.9910245504742486, 'raw_r2': 0.9873858297467437}\n",
      "Best loss: 0.063\n",
      "Mean loss: 0.075\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep: 1/  5. LR: 9.970e-05. Loss: 0.2577. Stats01: 0.0000:  30%|██▉       | 1044/3529 [13:02<24:16,  1.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rmse': 0.08524057165902361, 'raw_rmse': 40.57053167094432, 'mae': 0.06767289194614172, 'raw_mae': 15.411100935116227, 'r2': 0.989941747099362, 'raw_r2': 0.9926780250971825}\n",
      "Best loss: 0.063\n",
      "Mean loss: 0.074\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep: 1/  5. LR: 9.956e-05. Loss: 0.2647. Stats01: 0.0000:  35%|███▌      | 1248/3529 [15:03<22:57,  1.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rmse': 0.09094366000676074, 'raw_rmse': 39.57723923564668, 'mae': 0.07542725985521002, 'raw_mae': 16.270665450531553, 'r2': 0.9891697921924792, 'raw_r2': 0.9928097515168822}\n",
      "Best loss: 0.063\n",
      "Mean loss: 0.074\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep: 1/  5. LR: 9.941e-05. Loss: 0.2565. Stats01: 0.0000:  41%|████      | 1452/3529 [17:09<20:53,  1.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rmse': 0.08055922669108534, 'raw_rmse': 31.248443355673334, 'mae': 0.06365124810490805, 'raw_mae': 13.61684144595778, 'r2': 0.9920060462139344, 'raw_r2': 0.9958988468824975}\n",
      "Best loss: 0.063\n",
      "Mean loss: 0.073\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep: 1/  5. LR: 9.923e-05. Loss: 0.2548. Stats01: 0.0000:  47%|████▋     | 1656/3529 [19:11<15:57,  1.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rmse': 0.07570289287901687, 'raw_rmse': 35.35045638815908, 'mae': 0.05669932030978189, 'raw_mae': 13.69355228792736, 'r2': 0.9928167537044831, 'raw_r2': 0.9943104147224644}\n",
      "Best loss: 0.057\n",
      "Mean loss: 0.071\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep: 1/  5. LR: 9.914e-05. Loss: 0.2466. Stats01: 0.0000:  49%|████▉     | 1745/3529 [19:49<12:37,  2.36it/s]"
     ]
    }
   ],
   "source": [
    "best_model, best_loss = train_fold(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8216be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data, labels = test_fold[0], test_fold[1]\n",
    "eval_ensemble(args, best_model, data, labels, args.device, args.seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f003158",
   "metadata": {},
   "outputs": [],
   "source": [
    "data, labels = test_fold[0], test_fold[1]\n",
    "eval_ensemble(args, best_model, data, labels, args.device, [32, 64, 96])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6104b828",
   "metadata": {},
   "outputs": [],
   "source": [
    "data, labels = test_fold[0], test_fold[1]\n",
    "eval_ensemble(args, best_model, data, labels, args.device, [64, 96, 128])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee0812d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rlhf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
