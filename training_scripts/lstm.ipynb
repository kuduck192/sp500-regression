{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95c0efc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def manual_seed(seed=42):\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    # if you are suing GPU\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.enabled = False\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\n",
    "def get_model_size(model):\n",
    "\ttotal_size = sum(param.numel() for param in model.parameters() if param.requires_grad)\n",
    "\treturn total_size / 1e6\n",
    "\n",
    "manual_seed()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac10a513",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cffcd829",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import load_data, load_edge\n",
    "\n",
    "train_folds = load_data(True)\n",
    "test_fold = load_data(False)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "49da85ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "class SimpleStockDataset(Dataset):\n",
    "    def __init__(self, data, ws=128):\n",
    "        self.data = data\n",
    "        self.ws = ws\n",
    "        self.samples = []\n",
    "        \n",
    "        self.n_tickers, self.n_days, self.n_features = self.data.shape\n",
    "        \n",
    "        for start in range(self.n_days - self.ws + 1):\n",
    "            self.samples.append(start)\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "      \n",
    "    def __getitem__(self, idx):\n",
    "        start = self.samples[idx]\n",
    "        x = torch.tensor(self.data[:, start:start + self.ws], dtype=torch.float32)\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df3951fd",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6be37d6",
   "metadata": {},
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1edb5714",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from __future__ import annotations\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, Tuple\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class MyRNN(nn.Module):\n",
    "    def __init__(self, n_nodes: int, n_feats: int, args, node_emb=None):\n",
    "        super().__init__()\n",
    "        self.n_nodes = n_nodes\n",
    "        self.rank_A = 32\n",
    "        self.n_feats = n_feats\n",
    "        self.args = args\n",
    "        self.d_latent = getattr(args, \"d_latent\", 128)\n",
    "        self.dropout = getattr(args, \"dropout\", 0.0)\n",
    "        self.n_layers = getattr(args, \"n_layers\", 1)\n",
    "        self.d_node = getattr(args, \"d_node\", 32)\n",
    "        \n",
    "        \n",
    "\n",
    "        if node_emb is None:\n",
    "            self.static_node_features = nn.Parameter(torch.randn(n_nodes, self.d_latent) * 0.1)\n",
    "            self.fc_node_features = nn.Identity()\n",
    "        else:\n",
    "            self.static_node_features = nn.Parameter(node_emb, requires_grad=False)\n",
    "            self.d_node = self.static_node_features.shape[-1]\n",
    "            self.fc_node_features = nn.Linear(self.d_node, self.d_latent)\n",
    "        \n",
    "        self.cells = nn.ModuleList([\n",
    "            nn.LSTMCell(n_feats if i == 0 else self.d_latent, self.d_latent)\n",
    "            for i in range(self.n_layers)\n",
    "        ])\n",
    "        \n",
    "        self.readout = nn.Sequential(\n",
    "            nn.Linear(self.d_latent, self.d_latent), nn.ReLU(),\n",
    "            nn.Linear(self.d_latent, n_feats)\n",
    "        )\n",
    "        \n",
    "        self.enc_in = None\n",
    "\n",
    "    def _init_states(\n",
    "        self, X_0: torch.Tensor, H_0: Optional[torch.Tensor]\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Trả về list (h_list, c_list) cho từng layer.\n",
    "        - Nếu H_0 cung cấp (N, d), đặt h_top = H_0; ngược lại nếu enc_in != None thì h_top = enc(X_0); còn lại h=0.\n",
    "        - c luôn = 0.\n",
    "        \"\"\"\n",
    "        device = X_0.device\n",
    "        dtype = X_0.dtype\n",
    "        N = X_0.shape[0]\n",
    "\n",
    "        h_list = []\n",
    "        c_list = []\n",
    "        for _ in range(self.n_layers):\n",
    "            h_list.append(self.node_features)\n",
    "            c_list.append(self.node_features)\n",
    "\n",
    "        if H_0 is not None:\n",
    "            h_list[-1] = H_0.to(device=device, dtype=dtype)\n",
    "        elif self.enc_in is not None:\n",
    "            h_list[-1] = self.enc_in(X_0)\n",
    "\n",
    "        return h_list, c_list\n",
    "\n",
    "    def _step(self, x_t: torch.Tensor, h_list, c_list):\n",
    "        \"\"\"\n",
    "        Chạy 1 bước LSTM qua tất cả các layer.\n",
    "        Input layer nhận x_t; các layer sau nhận h của layer trước ở cùng time-step.\n",
    "        Trả về (h_list_new, c_list_new, h_top).\n",
    "        \"\"\"\n",
    "        inp = x_t\n",
    "        new_h, new_c = [], []\n",
    "        for l, cell in enumerate(self.cells):\n",
    "            h_t, c_t = cell(inp, (h_list[l], c_list[l]))\n",
    "            new_h.append(h_t)\n",
    "            new_c.append(c_t)\n",
    "            inp = h_t\n",
    "        h_top = new_h[-1]\n",
    "        if self.training and self.dropout > 0:\n",
    "            h_top = F.dropout(h_top, p=self.dropout, training=True)\n",
    "        return new_h, new_c, h_top\n",
    "\n",
    "    def forecast(self, X, H_0=None, horizon=0):\n",
    "        y, *_ = self.forward(X, H_0, horizon)\n",
    "        return y\n",
    "\n",
    "    def forward(self, X: torch.Tensor,\n",
    "                H_0: Optional[torch.Tensor] = None,\n",
    "                horizon: int = 0) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"Run deterministic inference.\n",
    "        Args:\n",
    "            X: (N, T, F) input features at each t\n",
    "            H_0: (N, d) optional initial latent state (default zeros)\n",
    "            Atilde, L: if precomputed; otherwise build from X\n",
    "        Returns:\n",
    "            y_pred: (N, T-1) predicted next-day log-return for each node\n",
    "            H_all: (N, T, d) latent states (including H_0 as first)\n",
    "        \"\"\"\n",
    "        device = self.args.device\n",
    "        X = X.to(next(self.parameters()).device)\n",
    "        n_nodes, n_steps, n_feats = X.shape\n",
    "        d_latent = self.d_latent\n",
    "        \n",
    "        self.node_features = self.fc_node_features(self.static_node_features)\n",
    "\n",
    "        X_0 = X[:, 0, :] # (N, F)\n",
    "        h_list, c_list = self._init_states(X_0, H_0)\n",
    "\n",
    "        H_all = torch.zeros(n_nodes, n_steps + max(horizon, 0), d_latent, device=device, dtype=X.dtype)\n",
    "        r_all = torch.zeros(n_nodes, n_steps - 1 + max(horizon, 0), n_feats, device=device, dtype=X.dtype)\n",
    "        y_all = torch.zeros(n_nodes, n_steps - 1 + max(horizon, 0), n_feats, device=device, dtype=X.dtype)\n",
    "\n",
    "\n",
    "        H_all[:, 0] = h_list[-1]\n",
    "        for t in range(n_steps - 1):\n",
    "            x_t = X[:, t, :]  # (N, F)\n",
    "            h_list, c_list, h_top = self._step(x_t, h_list, c_list)\n",
    "            r_t = self.readout(h_top)\n",
    "            y_t = x_t + r_t\n",
    "\n",
    "            H_all[:, t + 1] = h_top\n",
    "            r_all[:, t] = r_t\n",
    "            y_all[:, t] = y_t\n",
    "\n",
    "        if horizon > 0:\n",
    "            x_t = X[:, -1, :]\n",
    "            for s in range(horizon):\n",
    "                h_list, c_list, h_top = self._step(x_t, h_list, c_list)\n",
    "                r_t = self.readout(h_top)\n",
    "                y_t = x_t + r_t\n",
    "\n",
    "                idx = (n_steps - 1) + s\n",
    "                y_all[:, idx] = y_t\n",
    "                r_all[:, idx] = r_t\n",
    "                H_all[:, n_steps + s] = h_top\n",
    "\n",
    "                x_t = y_t\n",
    "\n",
    "        return y_all, r_all, H_all\n",
    "\n",
    "    def forward_loss(\n",
    "        self,\n",
    "        X: torch.Tensor,\n",
    "        H_0: Optional[torch.Tensor] = None,\n",
    "        horizon: int = 0,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Tính loss dự báo one-step + rollout horizon (autoregressive) trên chuỗi đầu vào.\n",
    "\n",
    "        Args:\n",
    "            X: (N, T, F) chuỗi gốc (chứa full ground-truth đến T-1)\n",
    "            H_0: (N, d) latent init (tuỳ chọn)\n",
    "            horizon: số bước rollout ngoài quan sát cuối cùng\n",
    "                    (nếu >0, ta cắt input để tránh nhìn thấy tương lai)\n",
    "            reduction: 'mean' | 'sum' | 'none' cho F.mse_loss\n",
    "\n",
    "        Returns:\n",
    "            loss: scalar tensor\n",
    "            y_pred: (N, T-1, F) dự báo X_{t+1} cho toàn bộ t=0..T-2\n",
    "        \"\"\"\n",
    "        device = next(self.parameters()).device\n",
    "        X = X.to(device)\n",
    "        N, T, Fdim = X.shape\n",
    "\n",
    "        if horizon < 0:\n",
    "            raise ValueError(\"horizon must be >= 0\")\n",
    "        if horizon >= T:\n",
    "            raise ValueError(f\"horizon={horizon} must be < sequence length T={T}\")\n",
    "\n",
    "        # Cắt input nếu rollout > 0 để giữ đúng số target (T-1)\n",
    "        X_in = X[:, : T - horizon, :] if horizon > 0 else X\n",
    "\n",
    "        # forward() trả (N, (T-horizon)-1 + horizon, F) = (N, T-1, F)\n",
    "        y_refine, _, y_ar, _, _ = self.forward(X_in, H_0=H_0, horizon=horizon)\n",
    "\n",
    "        # Ground truth luôn là X_{1:T}\n",
    "        target = X[:, 1:, :]  # (N, T-1, F)\n",
    "        \n",
    "        err_t = (y_ar - target).abs().mean(dim=-1).mean(dim=0)\n",
    "        \n",
    "        decay = 0.9\n",
    "        coef_pre = 1.0\n",
    "        coef_roll = 1.0\n",
    "        \n",
    "        len_pre = T - horizon - 1\n",
    "        loss_pre = torch.tensor(0.0, device=device, dtype=err_t.dtype)\n",
    "        loss_roll = torch.tensor(0.0, device=device, dtype=err_t.dtype)\n",
    "\n",
    "        if len_pre > 0:\n",
    "            idx = torch.arange(len_pre - 1, -1, -1, device=device, dtype=err_t.dtype)\n",
    "            w = decay ** idx\n",
    "            loss_pre = (w * err_t[:len_pre]).sum() / (w.sum() + 1e-12)\n",
    "\n",
    "        if horizon > 0:\n",
    "            # MSE đều cho đoạn rollout (chiều dài = horizon)\n",
    "            loss_roll = err_t[len_pre:].mean()\n",
    "\n",
    "        loss = coef_pre * loss_pre + coef_roll * loss_roll\n",
    "        \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd4f0de",
   "metadata": {},
   "source": [
    "# Eval & Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5a8ff2d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import *\n",
    "\n",
    "def eval_ensemble(args, model, training_data_np, testing_data_np, device='cuda', seq_lens=[64, 96, 128], verbose=False):\n",
    "\n",
    "    labels = torch.tensor(testing_data_np).float().to(device)\n",
    "    n_nodes, horizon, n_feats = labels.shape\n",
    "    y_preds = np.zeros((len(seq_lens), n_nodes, horizon, n_feats-1)) # Bỏ Vol\n",
    "    model.eval().to(device)\n",
    "    for i, seq_len in enumerate(seq_lens):\n",
    "        batch = torch.tensor(training_data_np[:, -seq_len:]).float().to(device)\n",
    "        with torch.no_grad():\n",
    "            y_all = model.forecast(batch, horizon=horizon)\n",
    "        y_preds[i] = y_all[:, -horizon:, :n_feats-1].detach().cpu().numpy() # OHLC + Adj Close\n",
    "    y_gt = testing_data_np[:, :, :n_feats-1].reshape(n_nodes, -1) # OHLC + Adj Close\n",
    "    y_pred = y_preds.mean(axis=0).reshape(n_nodes, -1)\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"Max var:\", np.var(np.expm1(y_preds), axis=0).max())\n",
    "        print(\"Mean var:\", np.var(np.expm1(y_preds), axis=0).mean())\n",
    "    return {\n",
    "        'rmse': root_mean_squared_error(y_gt, y_pred), \n",
    "        'raw_rmse': root_mean_squared_error(np.expm1(y_gt), np.expm1(y_pred)), \n",
    "        'mae': mean_absolute_error(y_gt, y_pred), \n",
    "        'raw_mae': mean_absolute_error(np.expm1(y_gt), np.expm1(y_pred)), \n",
    "        'r2': r2_score(y_gt.ravel(), y_pred.ravel()),\n",
    "        'raw_r2': r2_score(np.expm1(y_gt).ravel(), np.expm1(y_pred).ravel()), \n",
    "    }\n",
    "\n",
    "def eval(args, model, training_data_np, testing_data_np, device='cuda'):\n",
    "    return eval_ensemble(args, model, training_data_np, testing_data_np, device, [args.seq_len])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fafc31ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\n",
    "       Imported from https://github.com/pytorch/examples/blob/master/imagenet/main.py#L247-L262\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        if self.avg == 0:\n",
    "            self.avg = val\n",
    "            return\n",
    "        self.avg = 0.95 * self.avg + 0.05 * val\n",
    "\n",
    "import copy\n",
    "def train(args, train_loader, model, optimizer, scheduler, training_data_np, testing_data_np):\n",
    "    # if args.amp:\n",
    "    #     from apex import amp\n",
    "    global best_loss, best_model\n",
    "    test_losses = []\n",
    "    end = time.time()\n",
    "\n",
    "    best_model = copy.deepcopy(model)\n",
    "    step = 0\n",
    "    for epoch in range(args.epochs):\n",
    "        batch_time = AverageMeter()\n",
    "        data_time = AverageMeter()\n",
    "        losses = AverageMeter()\n",
    "        stats01 = AverageMeter()\n",
    "        stats02 = AverageMeter()\n",
    "        p_bar = tqdm(train_loader)\n",
    "        for batch_idx, samples in enumerate(p_bar):\n",
    "            step += 1\n",
    "            model.train().to(args.device)\n",
    "          \n",
    "            samples = samples[0].float().to(args.device)\n",
    "            data_time.update(time.time() - end)\n",
    "\n",
    "            loss = model.forward_loss(samples, horizon=args.horizon)\n",
    "            \n",
    "            loss.backward()\n",
    "            \n",
    "            max_norm = 5.0\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm).item()\n",
    "            \n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            losses.update(loss.item())\n",
    "            stats01.update(0.0)\n",
    "            # stats01.update(power_iteration_lmax_sym(poly_laplacian(model.L, F.softplus(model.kappa))))\n",
    "\n",
    "            batch_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "            # mask_probs.update(mask.mean().item())\n",
    "            p_bar.set_description(\n",
    "                \"Ep: {epoch}/{epochs:3}. LR: {lr:.3e}. \"\n",
    "                \"Loss: {loss:.4f}. Stats01: {stats01:.4f}\".format(\n",
    "                epoch=epoch + 1,\n",
    "                epochs=args.epochs,\n",
    "                lr=scheduler.get_last_lr()[0],\n",
    "                data=data_time.avg,\n",
    "                bt=batch_time.avg,\n",
    "                loss=losses.avg,\n",
    "                stats01=stats01.avg,\n",
    "            ))\n",
    "            p_bar.update()\n",
    "            \n",
    "            if (step + 1) % args.eval_steps == 0:\n",
    "                test_model = model\n",
    "\n",
    "                test_metrics = eval_ensemble(args, test_model, training_data_np, testing_data_np, args.device, [args.seq_len])\n",
    "                print(test_metrics)\n",
    "                test_loss = test_metrics['mae']\n",
    "\n",
    "                is_best = test_loss < best_loss\n",
    "                if test_loss < best_loss:\n",
    "                    best_loss = test_loss\n",
    "                    best_model = copy.deepcopy(test_model)\n",
    "\n",
    "\n",
    "                test_losses.append(test_loss)\n",
    "                print('Best loss: {:.3f}'.format(best_loss))\n",
    "                print('Mean loss: {:.3f}\\n'.format(\n",
    "                    np.mean(test_losses[-20:])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3c4bc09d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "import math\n",
    "\n",
    "def get_cosine_schedule_with_warmup(optimizer,\n",
    "                                    num_warmup_steps,\n",
    "                                    num_training_steps,\n",
    "                                    num_cycles=7./16.,\n",
    "                                    last_epoch=-1):\n",
    "    def _lr_lambda(current_step):\n",
    "        if current_step < num_warmup_steps:\n",
    "            return float(current_step) / float(max(1, num_warmup_steps))\n",
    "        no_progress = float(current_step - num_warmup_steps) / \\\n",
    "            float(max(1, num_training_steps - num_warmup_steps))\n",
    "        return max(0., math.cos(math.pi * num_cycles * no_progress))\n",
    "\n",
    "    return LambdaLR(optimizer, _lr_lambda, last_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d6ce9a9",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c25b9810",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def manual_seed(seed=42):\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    # if you are suing GPU\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.enabled = False\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\n",
    "def get_model_size(model):\n",
    "\ttotal_size = sum(param.numel() for param in model.parameters() if param.requires_grad)\n",
    "\treturn total_size / 1e6\n",
    "\n",
    "class Config:\n",
    "    # Training\n",
    "    epochs = 5\n",
    "    eval_steps = 200\n",
    "    lr = 1e-4\n",
    "    wd = 1e-3\n",
    "    warmup = 0\n",
    "    \n",
    "    n_nodes = 428\n",
    "    n_feats = 6\n",
    "    \n",
    "    # Prediction\n",
    "    seq_len = 96\n",
    "    horizon = 32\n",
    "    \n",
    "    # LSTM\n",
    "    d_node: int = None\n",
    "    d_latent: int = 128\n",
    "    device: str = \"cuda\"\n",
    "    n_layers = 1\n",
    "    dropout = 0.0\n",
    "    seed = 42\n",
    "\n",
    "args = Config()\n",
    "manual_seed(args.seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0adbbaf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "node_emb_path=\"../input/static_node_emb.npy\"\n",
    "node_emb_matrix = torch.tensor(np.load(node_emb_path), dtype=torch.float32)\n",
    "\n",
    "\n",
    "def train_fold(fold_idx):\n",
    "    global training_data_np, testing_data_np\n",
    "    used_features = [0, 1, 2, 3, 4, 5]\n",
    "    training_data_np = np.log1p(train_folds[fold_idx][0][:, :, used_features])\n",
    "    testing_data_np = np.log1p(train_folds[fold_idx][1][:, :, used_features])\n",
    "    train_loader = DataLoader(SimpleStockDataset(training_data_np, args.seq_len + args.horizon), batch_size=1, shuffle=True)\n",
    "    \n",
    "    manual_seed(args.seed + fold_idx)\n",
    "        \n",
    "    model = MyRNN(args.n_nodes, args.n_feats, args, node_emb=node_emb_matrix)\n",
    "\n",
    "    from torch.optim import AdamW\n",
    "\n",
    "    optimizer = AdamW(model.parameters(), lr=args.lr, weight_decay=args.wd)\n",
    "    total_steps = args.epochs * len(train_loader)\n",
    "    scheduler = get_cosine_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=args.warmup,\n",
    "        num_training_steps=total_steps,\n",
    "    )\n",
    "    \n",
    "    print(f'Model size: {get_model_size(model) * 1e3:.2f}K')\n",
    "    \n",
    "    # Sanity check\n",
    "    print(eval_ensemble(args, model, training_data_np, testing_data_np, args.device, [2, 4]))\n",
    "    \n",
    "    global best_loss, best_model\n",
    "    best_loss = 9999\n",
    "    best_model = copy.deepcopy(model)\n",
    "\n",
    "    train(args, train_loader, model, optimizer, scheduler, training_data_np, testing_data_np)\n",
    "    \n",
    "    return best_model, best_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f5b49c3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model size: 89.35K\n",
      "{'rmse': 0.46888273229933636, 'raw_rmse': 228.89217237270444, 'mae': 0.45586857612719023, 'raw_mae': 109.66308902441565, 'r2': 0.6031935891446447, 'raw_r2': 0.5531780766624103}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3529 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 5, got 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m best_model, best_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_fold\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[34], line 35\u001b[0m, in \u001b[0;36mtrain_fold\u001b[0;34m(fold_idx)\u001b[0m\n\u001b[1;32m     32\u001b[0m best_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m9999\u001b[39m\n\u001b[1;32m     33\u001b[0m best_model \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(model)\n\u001b[0;32m---> 35\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining_data_np\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtesting_data_np\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m best_model, best_loss\n",
      "Cell \u001b[0;32mIn[31], line 51\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(args, train_loader, model, optimizer, scheduler, training_data_np, testing_data_np)\u001b[0m\n\u001b[1;32m     48\u001b[0m samples \u001b[38;5;241m=\u001b[39m samples[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mto(args\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     49\u001b[0m data_time\u001b[38;5;241m.\u001b[39mupdate(time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m end)\n\u001b[0;32m---> 51\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhorizon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhorizon\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     55\u001b[0m max_norm \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5.0\u001b[39m\n",
      "Cell \u001b[0;32mIn[29], line 177\u001b[0m, in \u001b[0;36mMyRNN.forward_loss\u001b[0;34m(self, X, H_0, horizon)\u001b[0m\n\u001b[1;32m    174\u001b[0m X_in \u001b[38;5;241m=\u001b[39m X[:, : T \u001b[38;5;241m-\u001b[39m horizon, :] \u001b[38;5;28;01mif\u001b[39;00m horizon \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m X\n\u001b[1;32m    176\u001b[0m \u001b[38;5;66;03m# forward() trả (N, (T-horizon)-1 + horizon, F) = (N, T-1, F)\u001b[39;00m\n\u001b[0;32m--> 177\u001b[0m y_refine, _, y_ar, _, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(X_in, H_0\u001b[38;5;241m=\u001b[39mH_0, horizon\u001b[38;5;241m=\u001b[39mhorizon)\n\u001b[1;32m    179\u001b[0m \u001b[38;5;66;03m# Ground truth luôn là X_{1:T}\u001b[39;00m\n\u001b[1;32m    180\u001b[0m target \u001b[38;5;241m=\u001b[39m X[:, \u001b[38;5;241m1\u001b[39m:, :]  \u001b[38;5;66;03m# (N, T-1, F)\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 5, got 3)"
     ]
    }
   ],
   "source": [
    "best_model, best_loss = train_fold(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8216be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data, labels = test_fold[0], test_fold[1]\n",
    "eval_ensemble(args, best_model, data, labels, args.device, args.seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f003158",
   "metadata": {},
   "outputs": [],
   "source": [
    "data, labels = test_fold[0], test_fold[1]\n",
    "eval_ensemble(args, best_model, data, labels, args.device, [32, 64, 96])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6104b828",
   "metadata": {},
   "outputs": [],
   "source": [
    "data, labels = test_fold[0], test_fold[1]\n",
    "eval_ensemble(args, best_model, data, labels, args.device, [64, 96, 128])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee0812d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rlhf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
